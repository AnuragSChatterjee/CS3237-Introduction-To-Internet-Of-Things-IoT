# -*- coding: utf-8 -*-
"""A0200533Y_Anurag Chatterjee_&_A0200521E_Aryan Saraswat_CS3237 Lab 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VTR_LrLsR595zYz2oM6o2e7d6ZBOvrQ5

# CS3237 Lab 4 - Neural Networks

|Student Nunmber|Name                  |
|:--------------|:---------------------|
|   | Aryan Sarswat |
|   | Anurag S. Chatterjee |

Please work together as a team of 2 to complete this lab. You will need to submit ONE copy of this notebook per team, but please fill in the names of both team members above. This lab is worth 25 marks:

**DO NOT SUBMIT MORE THAN ONE COPY OF THIS LAB!**


## 1. Introduction

The objectives of this lab are:

    1. To familiarize you with how to create dense neural networks using Keras.
    2. To familiarize you with how to encode input and output vectors for neural networks.
    3. To give you some insight into how hyperparameters like learning rate and momentum affect training.
    
To save time we will train each experiment only for 50 epochs. This will lead to less than optimal results but is enough for you to make observations.

**HINT: YOU CAN HIT SHIFT-ENTER TO RUN EACH CELL. NOTE THAT IF A CELL IS DEPENDENT ON A PREVIOUS CELL, YOU WILL NEED TO RUN THE PREVIOUS CELL(S) FIRST **


## 2. The Irises Dataset

We will now work again on the Irises Dataset, which we used in Lab 3, for classifying iris flowers into one of three possible types. As before we will consider four factors:

    1. Sepal length in cm
    2. Sepal width in cm
    3. Petal length in cm
    4. Petal width in cm

In this dataset there are 150 sample points. The code below loads the dataset and prints the first 10 rows so we have an idea of what it looks like.
"""

from sklearn.datasets import load_iris
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

iris = load_iris()

print("First 10 rows of data:")
print(iris.data[:10])

"""### 2.2 Scaling the Data

We make use of the MinMaxScaler to scale the inputs to between 0 and 1.  The code below does this and prints the first 10 rows again, to show us the difference.

In the next section we will investigate what happens if we use unscaled data.
"""

scaler = MinMaxScaler()
scaler.fit(iris.data)
X = scaler.transform(iris.data)

print("First 10 rows of SCALED data.")
print(X[:10])

"""### 2.3 Encoding the Targets

In Lab 3 we saw that the target values (type of iris flower) is a vector from 0 to 2. We can see the 150 labels below:

"""

print(iris.target)

!pip install tensorflow

"""We can use this to train the neural network, but we will use "one-hot" encoding, where we have a vector of _n_ integers consisting of 0's and 1's.  The table below shows how one-hot encoding works:

|   Value    |    One-Hot Encoding    |
|:----------:|:----------------------:|
| 0 | \[1 0 0\] |
| 1 | \[0 1 0\] |
| 2 | \[0 0 1\] |

Keras provides the to_categorical function to create one-hot vectors:


"""

!pip install keras

from tensorflow.keras.utils import to_categorical

Y = to_categorical(y = iris.target, num_classes = 3)
print(Y)

"""Now let's split the data into training and testing data:


"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, 
                                                    random_state = 1)

"""### 2.4 Building our Neural Network

Let's now begin building a simple neural network with a single hidden layer, using the Stochastic Gradient Descent (SGD) optimizer, ReLu transfer functions for the hidden layer and softmax for the output layer.

The code to do this is shown below:
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD

# Create the neural network
nn = Sequential()
nn.add(Dense(100, input_shape = (4, ), activation = 'relu'))
nn.add(Dense(3, activation = 'softmax'))

# Create our optimizer
sgd = SGD(learning_rate=0.1, momentum=1.0, nesterov=False)

# 'Compile' the network to associate it with a loss function,
# an optimizer, and what metrics we want to track
nn.compile(loss='categorical_crossentropy', optimizer=sgd, 
          metrics = 'accuracy')

"""### 2.5 Training the Neural Network

As is usually the case, we can call the "fit" method to train the neural network for 50 epochs. We will shuffle the training data between epochs, and provide validation data.
"""

nn.fit(X_train, Y_train, shuffle = True, epochs = 50, 
      validation_data = (X_test, Y_test))

nn.evaluate(X_test, Y_test)

"""---
#### Question 1

Run the code above. Do you see evidence of underfitting? Overfitting? Justify your answers. ***(4 MARKS)***

**Answer: Overfitting occurs when the training accuracy is high but the validation accuracy is low, this is not observed in this case. Underfitting occurs when the training accuracy is low and the validation accuracy is low as well, this occurs because the model is two simplistic with too few parameters. The model is neither underfitting or overfitting as the training and validation i.e. testing accuracy levels are high as the number of epochs increase till 50. The training accuracy has increased from 0.5 to 0.9167 and the validation accuracy has increased from 0.5667 to 0.9667, so both validation and training acuracies are high in the case of epoch 50.**

_(For TA) Marks awarded: ____ / 4_

---

#### Question 2a

Consult the documentation for the SGD optimizer [here](https://keras.io/api/optimizers/sgd/). What does the lr parameter do? ***(1 MARK)***

**Answer: The lr parameter stands for learning_rate and it is multiplied to the derivative of the loss with respect to the parameters of the neural network to decide how much to update the the networks weight at each timestep. It is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.**

#### Question 2b

The documentation states that the momentum parameter "accelerates gradient descent in the relevant direction and dampens oscillations". Using Google or other means, illustrate what this means. ***(2 MARKS)***

**Answer: The momemtum parameter is a coeifficient which is multipied to the aggregate of the history of previous gradients. By taking the average of the previous gradient it ensures that the update to the neural network's parameters are in the right direction and they do not fluctuate (oscillate) too much [Since an aggregate is being taken].**

_(For TA) Marks awarded: ____ / 3_

----

#### Question 3a

We will now play with the lr parameter. Adjust the lr parameter to the following values and record the final training and validation accuracies in the respective columns. Also observe the sequence of accuracies over the training period, and place your observation in the "remarks" column, e.g. "Progresses steadily", "some oscillation" etc. ***(3 MARKS)***

**Answer: Fill the table below **

|  lr    | Training Acc. | Validation Acc. |      Remarks      |
|:------:|---------------|-----------------|-------------------|
|0.01    |      0.6917         |      0.5667           |       Progresses steadily            |
|0.1     |        0.9333       |      0.9667           |       Progresses steadily            |
|1.0     |       0.9583        |      0.9667           |       some oscillation            |
|10.0    |         0.3583      |          0.3667       |      some oscillation             |
|100     |        0.3833       |        0.3667         |      some oscillation             |
|1000    |     0.3333          |         0.4333        |      some oscillation             |
|10000   |     0.3417          |         0.4333        |      some oscillation             |
| 100000 |    0.3083           |        0.3667         |      some oscillation             |


#### Question 3b

Based on your observations above, comment on the effect of small and very large learning rates on the learning. ***(2 MARKS)***

**Answer: When the learning rate, lr, is too small, the model not learning fast enough as the gradient updates are very small, but when the learning rate is large, this leads to the gradient update being too large and the model misses the minima in the loss, thus leading to oscillation and lower accuracies as the model is not able to learn properly. **

_(For TA) Marks awarded: ____ / 5_

### 2.5 Using Momentum

We will now experiment with the momentum term. To do this:

    1. Change the learning rate to 0.1.
    2. Set the momentum to 0.1. Note: Do not use the Nesterov parameter - Leave it as False.
    
Run your neural network.

---

#### Question 4a

Keeping the learning rate at 0.1, complete the table below using the momentum values shown. Again record any observations in the "Remarks" column. ***(3 MARKS)***

**Answer: Fill the table below**

| momentum | Training Acc. | Validation Acc. |      Remarks      |
|:--------:|---------------|-----------------|-------------------|
|0.001     |     0.9167          |    0.9667             |    Progresses steadily                |
|0.01      |      0.9417         |      0.9667           |     Progresses steadily              |
|0.1       |      0.9583         |       0.9667          |      Progresses steadily             |
|1.0       |     0.9583          |       1.0000          |     some oscillation              |

#### Question 4b

Based on your observations above, does the momentum term help in learning? ***(2 MARKS)***

**Answer: Yes momentum, may help in training as it helps stabalise the gradient updates, preventing them from making incorrect erratic gradient updates, as shown by steadily decreasing loss during training and the minimal change in training and validation accuracies.**

_(For TA) Marks awarded: ____ / 5_

---

### 2.6 Using Raw Unscaled Data

We begin by using unscaled X and Y data. The code below will create 120 training samples and 30 testing samples (20% of the total of 150 samples):
"""

X_unscaled = iris.data
Y_raw = iris.target
X_utrain, X_utest, Y_utrain, Y_utest = train_test_split(X_unscaled, Y,
                                                        test_size = 0.2,
                                                        random_state = 1)

"""---

#### Question 5

Create a new neural network called "nn2" below using a single hidden layer of 100 neurons. Train using the data in X_utrain, X_utest and validate with Y_utrain and Y_utest. Again use the SGD optimizer with a learning rate of 0.1 and no momentum, and train for 50 epochs. ***(3 marks)***
"""

"""
Enter your code for Question 5 below. To TA: 3 marks for code.
"""
#Creating a neural network model naned nn2
nn2 = Sequential()
nn2.add(Dense(100, input_shape = (4, ), activation = 'relu'))
nn2.add(Dense(3, activation = 'softmax'))

# Create our optimizer
sgd = SGD(learning_rate=0.1, momentum=0.0, nesterov=False, decay = 0.0, name="SGD",)

# 'Compile' the network to associate it with a loss function,
# an optimizer, and what metrics we want to track
nn2.compile(loss='categorical_crossentropy', optimizer=sgd, 
          metrics = 'accuracy')

nn2.fit(X_utrain, Y_utrain, shuffle = True, epochs = 50, validation_data = (X_utest, Y_utest))

print("Done testing. Now evaluating:")

loss, acc = nn2.evaluate(X_utest, Y_utest)
print("Final loss is %3.2f, accuracy is %3.2f." % (loss, acc))

"""**(Question 5 continues)**

Observe the training and validation error. Does not scaling the input affect the training? Why do you think this is so? What is the advantage of scaling? ***(5 MARKS)***

**Answer: Scaling definitely affects training as now the network's weights are overfitting to one particular feature, as that one feature might have a large range compared to the other features leading to very high losses. This leads to the network prioritising one feature, which can be further seen as the validation accuracy is fluctuating a lot meaning that the network has not trained well. By scaling we are preventing the network from being biased towards a particular feature as they are all within the range of 0-1, and the loss with respect to each feature is weighed equally and allow for efficient training.**

_(For TA) Marks awarded: ____ / 8_

---

## 3 Conclusion

In this lab we saw how to create a simple Dense neural network to complete the relatively simple task of learning how to classify irises according to their sepal and petal characteristics. 


---

***FOR TA ONLY***

| Question |  Marks  |
|:--------:|:-------:|
|1         |     /4  |
|2         |     /3  |
|3         |     /5  |
|4         |     /5  |
|5         |     /8  |
|Total:    |     /25 |
"""

